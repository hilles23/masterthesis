{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing StereoSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data:\n",
    "stereo = pd.read_json('/home/hilles/stereoset/data/dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stereo['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the 'data' column\n",
    "flat_data = pd.json_normalize(stereo['data'][0])\n",
    "flat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_2 = pd.read_parquet(\"hf://datasets/McGill-NLP/stereoset/intersentence/validation-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatgpt\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load intersentence training split\n",
    "#intersentence_train = load_dataset(\"McGill-NLP/stereoset\", \"intersentence\", split=\"train\")\n",
    "#this gave an error, since huggingface doesnot include that split. Since it is for training, I am just ignoring that part\n",
    "\n",
    "# Load intrasentence validation split\n",
    "intrasentence_validation = load_dataset(\"McGill-NLP/stereoset\", \"intrasentence\", split=\"validation\")\n",
    "\n",
    "# Similarly, load test splits for intersentence and intrasentence\n",
    "#intersentence_test = load_dataset(\"McGill-NLP/stereoset\", \"intersentence\", split=\"test\")\n",
    "#intrasentence_test = load_dataset(\"McGill-NLP/stereoset\", \"intrasentence\", split=\"test\")\n",
    "\n",
    "\n",
    "intersentence_validation = load_dataset(\"McGill-NLP/stereoset\", \"intersentence\", split=\"validation\")\n",
    "\n",
    "intrasentence_validation = load_dataset(\"McGill-NLP/stereoset\", \"intrasentence\", split=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = pd.read_csv('/home/hilles/intersentence_validation.csv')\n",
    "intra = pd.read_csv('/home/hilles/intrasentence_validation.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset = pd.concat([inter, intra], ignore_index=True)\n",
    "stereoset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stereoset['sentences'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(stereoset['sentences'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_sentences(row):\n",
    "    data = ast.literal_eval(row)\n",
    "    sentences = data['sentence'].tolist()\n",
    "    labels = data['labels']\n",
    "    gold_labels = data['gold_label'].tolist()\n",
    "    return pd.DataFrame({\n",
    "        \"sentence\": sentences,\n",
    "        \"label\": labels,\n",
    "        \"gold_label\": gold_labels\n",
    "    })\n",
    "\n",
    "# Apply parsing to extract components\n",
    "all_sentences = []\n",
    "for _, row in stereoset.iterrows():\n",
    "    all_sentences.append(parse_sentences(row['sentences']))\n",
    "\n",
    "# Combine all sentences into one DataFrame\n",
    "flat_sentences = pd.concat(all_sentences, ignore_index=True)\n",
    "print(flat_sentences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Define a parsing function\n",
    "def parse_sentences(row):\n",
    "    try:\n",
    "        # Use ast.literal_eval to convert the string into a Python dictionary\n",
    "        data = ast.literal_eval(row) \n",
    "        sentences = data['sentence']\n",
    "        labels = data['labels']\n",
    "        gold_labels = data['gold_label']\n",
    "        \n",
    "        # Create a DataFrame from the parsed data\n",
    "        return pd.DataFrame({\n",
    "            \"sentence\": sentences,\n",
    "            \"label\": labels,\n",
    "            \"gold_label\": gold_labels\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing row: {row}, Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Test with a single example\n",
    "example = stereoset['sentences'].iloc[0]\n",
    "parsed_example = parse_sentences(example)\n",
    "print(parsed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stereoset['target'].iloc[0])\n",
    "\n",
    "#tried out all the other columns, and apparently the other ones are alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset['sentences'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def expand_sentences(row):\n",
    "    # Parse the sentences dictionary\n",
    "    data = ast.literal_eval(row)\n",
    "    sentences = data['sentence']\n",
    "    ids = data['id']\n",
    "    labels = data['gold_label']\n",
    "    \n",
    "    # Create a DataFrame for expanded sentences\n",
    "    df = pd.DataFrame({\n",
    "        'sentence': sentences,\n",
    "        'sentence_id': ids,\n",
    "        'gold_label': labels,\n",
    "        'context': row['context'],\n",
    "        'bias_type': row['bias_type'],\n",
    "        'target': row['target']\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Apply the expansion function to each row in the StereoSet DataFrame\n",
    "expanded_sentences = pd.concat(\n",
    "    stereoset.apply(lambda row: expand_sentences(row['sentences']), axis=1),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# The expanded_sentences DataFrame now has one sentence per row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new try:\n",
    "\n",
    "def expand_sentences(row):\n",
    "    try:\n",
    "        data = ast.literal_eval(row)  # Parse the sentence dictionary string\n",
    "        sentences = data['sentence']\n",
    "        ids = data['id']\n",
    "        labels = data['gold_label']\n",
    "        \n",
    "        # Create a DataFrame for the expanded sentences\n",
    "        df = pd.DataFrame({\n",
    "            'sentence': sentences,\n",
    "            'sentence_id': ids,\n",
    "            'gold_label': labels,\n",
    "            'context': row['context'],\n",
    "            'bias_type': row['bias_type'],\n",
    "            'target': row['target']\n",
    "        })\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing row: {row}. Error: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame for problematic rows\n",
    "\n",
    "# Apply the expansion function\n",
    "expanded_sentences = pd.concat(\n",
    "    stereoset.apply(lambda row: expand_sentences(row['sentences']), axis=1),\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereoset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stereoset['bias_type'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
