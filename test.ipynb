{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "# Lade das BBQ-Dataset\n",
    "def load_bbq_dataset(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Preprocessing-Funktion: Korrigiere die Antwort basierend auf dem Label\n",
    "def preprocess_bbq_dataset(dataset):\n",
    "    for entry in dataset:\n",
    "        label = entry.get(\"label\")\n",
    "        if label == 0:\n",
    "            # Keine Änderungen nötig\n",
    "            pass\n",
    "        elif label == 1:\n",
    "            # Tausche ans0 und ans1\n",
    "            entry[\"ans0\"], entry[\"ans1\"] = entry[\"ans1\"], entry[\"ans0\"]\n",
    "        elif label == 2:\n",
    "            # Wenn es weitere Labels gibt, könnte hier Logik ergänzt werden\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekanntes Label: {label}\")\n",
    "    return dataset\n",
    "\n",
    "# Funktion, um das Modell zu analysieren\n",
    "def analyze_bias_with_llm(dataset, model_name=\"gpt-3.5-turbo\"):\n",
    "    # Initialisiere das LLM (z.B., mit Hugging Face)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        context = entry[\"context\"]\n",
    "        ans0 = entry[\"ans0\"]\n",
    "        ans1 = entry[\"ans1\"]\n",
    "\n",
    "        # Optional: Kontext integrieren, falls nötig\n",
    "        full_context = f\"{context}\\n{question}\"\n",
    "\n",
    "        # Modell-Antwort generieren\n",
    "        model_response = qa_pipeline(question=question, context=context)\n",
    "\n",
    "        # Prüfe, ob die Antwort stereotypisch oder anti-stereotypisch ist\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"model_answer\": model_response[\"answer\"],\n",
    "            \"stereotype\": ans0 if model_response[\"answer\"] == ans0 else ans1,\n",
    "            \"confidence\": model_response[\"score\"]\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Ergebnisse analysieren und aggregieren\n",
    "def aggregate_results(results):\n",
    "    stereotype_count = 0\n",
    "    total = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        if result[\"stereotype\"] == result[\"model_answer\"]:\n",
    "            stereotype_count += 1\n",
    "\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"stereotype_count\": stereotype_count,\n",
    "        \"stereotype_percentage\": stereotype_count / total * 100\n",
    "    }\n",
    "\n",
    "# Beispielhafter Pfad zum BBQ-Dataset\n",
    "bbq_dataset_path = \"path/to/bbq_dataset.json\"\n",
    "\n",
    "# Lade das Dataset\n",
    "data = load_bbq_dataset(bbq_dataset_path)\n",
    "\n",
    "# Preprocessing durchführen\n",
    "data = preprocess_bbq_dataset(data)\n",
    "\n",
    "# Bias-Analyse durchführen\n",
    "results = analyze_bias_with_llm(data, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Ergebnisse zusammenfassen\n",
    "summary = aggregate_results(results)\n",
    "\n",
    "# Ausgabe der Analyse\n",
    "print(\"Ergebnisse der Bias-Analyse:\")\n",
    "print(f\"Anzahl der Fragen: {summary['total']}\")\n",
    "print(f\"Stereotype Antworten: {summary['stereotype_count']}\")\n",
    "print(f\"Prozentsatz der stereotype Antworten: {summary['stereotype_percentage']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
